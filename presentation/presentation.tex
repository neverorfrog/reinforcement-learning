\documentclass{beamer}
\usepackage{amsfonts,amsmath,oldgerm}
\usetheme{sintef}

\newcommand{\testcolor}[1]{\colorbox{#1}{\textcolor{#1}{test}}~\texttt{#1}}

\usefonttheme[onlymath]{serif}

\titlebackground*{assets/background}

\newcommand{\hrefcol}[2]{\textcolor{cyan}{\href{#1}{#2}}}

\title{Hindsight Goal Prioritization for Sparse Reward Environments}
\subtitle{Final Project}
\course{Reinforcement Learning}
\author{\href{mailto:maiorana.2051396@studenti.uniroma1.it}{Flavio Maiorana}}
\IDnumber{2051396}
\date{08/09/2023}

\begin{document}
\maketitle

\section{Introduction}

      \begin{frame}{Problem statement}
            \begin{block}{Robotics environments}
                  \begin{itemize}
                        \item Complex and different goals
                        \item Sparse rewards
                        \item Continuous action space
                  \end{itemize}  
            \end{block}

            $\Downarrow$ \centering \\

            \begin{block}{Problems with exploration and reward shaping}
                  \begin{itemize}
                        \item Goal may be too complex and observation space is big: we may never get reward 1
                        \item Classical off-policy algorithms don't valorize much the failed episodes
                  \end{itemize}  
            \end{block}

            $\Downarrow$ \centering \\

            Solution: Enhancing the Replay Buffer
      \end{frame}

      \begin{frame}{Fetch}
            \begin{itemize}
                  \item Based on the 7-DoF Fetch Manipulator arm, with a two-fingered parallel gripper
                  \item Tasks: Reach, Push, Slide and Pick-and-Place
                  \item Action: Box(-1.0, 1.0, (4,), float32) $\rightsquigarrow$ Displacement in meters of the EE 
                  \item Observation: dictionary with info about the robotâ€™s end effector state and goal
                        \begin{itemize}
                              \item Observation: ndarray of shape (25,) $\rightsquigarrow$ kinematic info of the block object and EE
                              \item Desired goal: ndarray of shape (3,) $\rightsquigarrow$ desired position of the EE or the block
                              \item Achieved goal: ndarray of shape (3,) $\rightsquigarrow$ current position of the EE or the block
                        \end{itemize}
                  \item Reward: if we use sparse rewards -1 for every timestep and 0 for reaching the goal
                  \item Termination: episodes have no termination since they have infinite horizon. Thus, they are truncated after T steps (by default 50)
            \end{itemize}
      \end{frame}

\section{Algorithm comparison}

      \begin{frame}[fragile]{DDPG}
            \framesubtitle{The base of all}
            \begin{block}{Intuition}
                  \verb|\documentclass{beamer}|\\
            \end{block}
            \begin{block}{Drawbacks}
                  \verb|\documentclass{beamer}|\\
            \end{block}
      \end{frame}

      \begin{frame}[fragile]{HER}
            \framesubtitle{Sampling future goals}
            To start working with \texttt{sintefbeamer}
            \begin{block}{Intuition}
                  \verb|\documentclass{beamer}|\\
            \end{block}
            \begin{block}{Drawbacks}
                  \verb|\documentclass{beamer}|\\
            \end{block}
      \end{frame}

      \begin{frame}[fragile]{HGR}
            \framesubtitle{Prioritizing future goals}
            \begin{block}{Intuition}
                  \verb|\documentclass{beamer}|\\
            \end{block}
            \begin{block}{Enhancements over Vanilla HER}
                  \verb|\documentclass{beamer}|\\
            \end{block}
      \end{frame}


\section{Implementation}

      \begin{frame}{Learning Algorithm}
                  
      \end{frame}

      \begin{frame}{Replay Buffer}
                  
      \end{frame}


\section{Results}

\backmatter
\end{document}