\documentclass{beamer}
\usepackage{amsfonts,amsmath,oldgerm}
\usetheme{sintef}

\newcommand{\testcolor}[1]{\colorbox{#1}{\textcolor{#1}{test}}~\texttt{#1}}

\usefonttheme[onlymath]{serif}

\titlebackground*{assets/background}

\newcommand{\hrefcol}[2]{\textcolor{cyan}{\href{#1}{#2}}}

\title{Hindsight Goal Prioritization for Sparse Reward Environments}
\subtitle{Final Project}
\course{Reinforcement Learning}
\author{\href{mailto:maiorana.2051396@studenti.uniroma1.it}{Flavio Maiorana}}
\IDnumber{2051396}
\date{08/09/2023}

\begin{document}
\maketitle

\section{Introduction}

      \begin{frame}{Problem statement}
            \begin{block}{Robotics environments}
                  \begin{itemize}
                        \item Complex and different goals
                        \item Sparse rewards
                        \item Continuous action space
                  \end{itemize}  
            \end{block}

            $\Downarrow$ \centering \\

            \begin{block}{Problems with exploration and reward shaping}
                  \begin{itemize}
                        \item Goal may be too complex and observation space is big: we may never get reward 1
                        \item Classical off-policy algorithms don't valorize much the failed episodes
                  \end{itemize}  
            \end{block}

            $\Downarrow$ \centering \\

            Solution: Enhancing the Replay Buffer
      \end{frame}

      \begin{frame}{Fetch}
            \begin{itemize}
                  \item Based on the 7-DoF Fetch Manipulator arm, with a two-fingered parallel gripper
                  \item Tasks: Reach, Push, Slide and Pick-and-Place
                  \item Action: Box(-1.0, 1.0, (4,), float32) $\rightsquigarrow$ Displacement in meters of the EE 
                  \item Observation: dictionary with info about the robotâ€™s end effector state and goal
                        \begin{itemize}
                              \item Observation: ndarray of shape (25,) $\rightsquigarrow$ kinematic info of the block object and EE
                              \item Desired goal: ndarray of shape (3,) $\rightsquigarrow$ desired position of the EE or the block
                              \item Achieved goal: ndarray of shape (3,) $\rightsquigarrow$ current position of the EE or the block
                        \end{itemize}
                  \item Reward: if we use sparse rewards -1 for every timestep and 0 for reaching the goal
                  \item Termination: episodes have no termination since they have infinite horizon. Thus, they are truncated after T steps (by default 50)
            \end{itemize}
      \end{frame}

\section{DDPG}

      \begin{frame}[fragile]{Architecture}
            \begin{itemize}
                  \item Two neural networks in performing actor-critic policy gradient
                  \begin{columns}
                        \column{0.5\textwidth}
                        
                        \column{0.5\textwidth}
                        
                    \end{columns} 
                        \begin{itemize}
                              \item Actor inference: observed state $\rightarrow$ action maximising the action-value function
                              \item Critic inference: state and action $\rightarrow value of the action-value function
                        \end{itemize}
            \end{itemize}
      \end{frame}

      \begin{frame}{Implementation}
      \end{frame}

      \begin{frame}{Drawbacks in the Fetch Environment}
            \begin{itemize}
                  \item 
            \end{itemize}
      \end{frame}


\section{Replay buffer}

      \begin{frame}[fragile]{HER}
            \framesubtitle{Intuition}
            \begin{itemize}
                  \item The intention is to valorize also failed episodes (majority in robotics environments)
                  \item Done by storing episodes multiple times, substituting the desired goal with another from the same episode, treating the episode as if it was successful
                  \item Formally speaking, for each episode ($t = 0 ... 50$) we do the following steps
                        \begin{itemize}
                              \item store $\left( s_t||g, a_t, r_t, s_{t+1}||g\right)$
                              \item sample a set of additional achieved goals $G$ from the current episode
                              \item store $\left( s_t||g', a_t, r_t, s_{t+1}||g'\right)$ for every $g' \in G$
                        \end{itemize}
                  \item Different strategies can be adopted to sample goals
                        \begin{itemize}
                              \item Final
                              \item Future
                              \item Random
                        \end{itemize}
            \end{itemize}
      \end{frame}

      \begin{frame}[fragile]{HER}
            \framesubtitle{Implementation}

      \end{frame}



      \begin{frame}[fragile]{HGR}
            \framesubtitle{Prioritizing future goals}
            \begin{block}{Intuition}
                  \verb|\documentclass{beamer}|\\
            \end{block}
            \begin{block}{Enhancements over Vanilla HER}
                  \verb|\documentclass{beamer}|\\
            \end{block}
      \end{frame}


\section{Results}

\backmatter
\end{document}