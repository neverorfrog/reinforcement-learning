{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Definition\n",
    "- Concerns the study of ways to teach a dynamical system to take an action $a$ while living in state $s$ following a certain goal\n",
    "- Different from supervised learning because we don't have any labels\n",
    "- Different from unsupervised learning because we are interested in the time evolution of the system in order to maximise a reward, not just to discover a structure itself\n",
    "\n",
    "\n",
    "### Dynamic System\n",
    "- The elements are:\n",
    "    - State $x$: snapshot at a time instant\n",
    "    - Transition function $f$: from $x_k$ to $x_{k+1}$\n",
    "    - Observation function $h$: from $x_k$ to $z_k$\n",
    "- Assumptions\n",
    "    - Markov (**MDP**): the current state is a substitute for the entire history of the system\n",
    "    - Full observability: the observation model simply returns the state\n",
    "- Different problem paradigms\n",
    "    - Reasoning: getting the state evolution by knowing the transition function\n",
    "    - Learning: getting the transition function by knowing the state evolution\n",
    "- **Reinforcement Learning Problem applied to Dynamic System**\n",
    "    - Learning scenario\n",
    "    - Only using past experience, compute an action at each state\n",
    "    - The goal is to reach a final state (or maximise a reward)\n",
    "- To summarize, the goal is to learn the policy function $\\pi: S \\rightarrow A$\n",
    "\n",
    "### Formalizing MDP\n",
    "- Formed by an agent and an environment, interacting with each other, generating a trajectory\n",
    "    - $S_0, A_0, R_1, S_1, A_1, R_2, ..., S_n, A_n, R_{T+1}$\n",
    "    - in state t, by applying action t, i get rewardt t+1 \n",
    "- We assume the MDP is finite and fully-observable\n",
    "- $S$ finite set of states (coordinates of the agent in a grid)\n",
    "- $A$ finite set of actions (directions in which the agent can go)\n",
    "- Defined as a tuple $<S,A,f,r>$\n",
    "    - **Deterministic** MDP\n",
    "        - $f: S \\times A \\rightarrow S $: transition function returns $S_{t+1} = f(S_t,A_t)$\n",
    "        - $r: s \\times A \\rightarrow \\R$: reward function returns $r(S_t,A_t)$\n",
    "    - **Non-Deterministic** MDP\n",
    "        - $f: S \\times A \\rightarrow 2^{S} $: transition function return a set of possible alternatives, not just one state\n",
    "        - $r: s \\times A \\rightarrow \\R$: reward function\n",
    "        - Result of an action can be observed only AFTER the action is executed\n",
    "    - **Stochastic** MDP\n",
    "        - $f: S \\times A \\times S' \\times A' \\rightarrow [0,1]$ defined as transition from a probability distribution\n",
    "            - $f(s',r|s,a) = Pr(S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a)$\n",
    "        - From the expression above we can define everything else through marginalization and expected value\n",
    "            - $f(s'|s,a) = \\Sigma_r f(s',r|s,a) \\rightarrow$ probability of getting to new state $s'$\n",
    "            - $r(s,a) = \\mathbb{E}[R_t | S_{t-1}=s, A_{t-1}=a] = \\Sigma_r r \\Sigma_{s'} f(s',r|s,a)$ \n",
    "                - Expected Value $\\mathbb{E}[r|s,a]$ is $\\Sigma r f(r|s,a)$\n",
    "                - The second term is obtained by marginalization\n",
    "\n",
    "### Goal in RL\n",
    "- **We aim to find a policy such that the cumulative reward in the long run is maximised**\n",
    "- How do we define the policy?\n",
    "    - As a mapping from a state to an action\n",
    "    - $\\pi(a|s): A \\times S \\rightarrow [0,1]$\n",
    "    - Given the agent is in a certain state, the action a is executed with a certain probability\n",
    "- How do we define the cumulative reward?\n",
    "    - $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... + \\gamma^{T-t+1} R_{T}$\n",
    "    - Also called **discounted return**\n",
    "    - The discount $\\gamma$ is needed to give more importance to proximal timesteps\n",
    "    - We can also define it **recursively**: $G_t = R_{t+1} + \\gamma G_{t+1}$\n",
    "- But this return can actually not be computed on the spot, so we define the **expected return**\n",
    "    - This is because we have to consider all the possible returns over all possible evolutions from the current state\n",
    "    - $\\mathbb{E}_\\pi[G_t|S_t=s] = v_{\\pi}(s) \\rightarrow$ VALUE FUNCTION\n",
    "    - $\\mathbb{E}_\\pi[G_t|S_t=s,A_t=a] = q_{\\pi}(s,a) \\rightarrow$ ACTION-VALUE FUNCTION\n",
    "- In the end we need to find $\\pi^* = \\argmax_\\pi v_{\\pi}(s)$ $\\forall s$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
