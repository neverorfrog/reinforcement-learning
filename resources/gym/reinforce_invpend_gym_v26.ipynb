{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Training using REINFORCE for Mujoco\n",
        "\n",
        "<img src=\"file://_static/img/tutorials/reinforce_invpend_gym_v26_fig1.gif\" width=\"400\" alt=\"agent-environment-diagram\">\n",
        "\n",
        "This tutorial serves 2 purposes:\n",
        " 1. To understand how to implement REINFORCE [1] from scratch to solve Mujoco's InvertedPendulum-v4\n",
        " 2. Implementation a deep reinforcement learning algorithm with Gymnasium's v0.26+ `step()` function\n",
        "\n",
        "We will be using **REINFORCE**, one of the earliest policy gradient methods. Unlike going under the burden of learning a value function first and then deriving a policy out of it,\n",
        "REINFORCE optimizes the policy directly. In other words, it is trained to maximize the probability of Monte-Carlo returns. More on that later.\n",
        "\n",
        "**Inverted Pendulum** is Mujoco's cartpole but now powered by the Mujoco physics simulator -\n",
        "which allows more complex experiments (such as varying the effects of gravity).\n",
        "This environment involves a cart that can moved linearly, with a pole fixed on it at one end and having another end free.\n",
        "The cart can be pushed left or right, and the goal is to balance the pole on the top of the cart by applying forces on the cart.\n",
        "More information on the environment could be found at https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/\n",
        "\n",
        "**Training Objectives**: To balance the pole (inverted pendulum) on top of the cart\n",
        "\n",
        "**Actions**: The agent takes a 1D vector for actions. The action space is a continuous ``(action)`` in ``[-3, 3]``,\n",
        "where action represents the numerical force applied to the cart\n",
        "(with magnitude representing the amount of force and sign representing the direction)\n",
        "\n",
        "**Approach**: We use PyTorch to code REINFORCE from scratch to train a Neural Network policy to master Inverted Pendulum.\n",
        "\n",
        "An explanation of the Gymnasium v0.26+ `Env.step()` function\n",
        "\n",
        "``env.step(A)`` allows us to take an action 'A' in the current environment 'env'. The environment then executes the action\n",
        "and returns five variables:\n",
        "\n",
        "-  ``next_obs``: This is the observation that the agent will receive after taking the action.\n",
        "-  ``reward``: This is the reward that the agent will receive after taking the action.\n",
        "-  ``terminated``: This is a boolean variable that indicates whether or not the environment has terminated.\n",
        "-  ``truncated``: This is a boolean variable that also indicates whether the episode ended by early truncation, i.e., a time limit is reached.\n",
        "-  ``info``: This is a dictionary that might contain additional information about the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Policy Network\n",
        "\n",
        "<img src=\"file://_static/img/tutorials/reinforce_invpend_gym_v26_fig2.png\">\n",
        "\n",
        "We start by building a policy that the agent will learn using REINFORCE.\n",
        "A policy is a mapping from the current environment observation to a probability distribution of the actions to be taken.\n",
        "The policy used in the tutorial is parameterized by a neural network. It consists of 2 linear layers that are shared between both the predicted mean and standard deviation.\n",
        "Further, the single individual linear layers are used to estimate the mean and the standard deviation. ``nn.Tanh`` is used as a non-linearity between the hidden layers.\n",
        "The following function estimates a mean and standard deviation of a normal distribution from which an action is sampled. Hence it is expected for the policy to learn\n",
        "appropriate weights to output means and standard deviation based on the current observation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Policy_Network(nn.Module):\n",
        "    \"\"\"Parametrized Policy Network.\"\"\"\n",
        "\n",
        "    def __init__(self, obs_space_dims: int, action_space_dims: int):\n",
        "        \"\"\"Initializes a neural network that estimates the mean and standard deviation\n",
        "         of a normal distribution from which an action is sampled from.\n",
        "\n",
        "        Args:\n",
        "            obs_space_dims: Dimension of the observation space\n",
        "            action_space_dims: Dimension of the action space\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        hidden_space1 = 16  # Nothing special with 16, feel free to change\n",
        "        hidden_space2 = 32  # Nothing special with 32, feel free to change\n",
        "\n",
        "        # Shared Network\n",
        "        self.shared_net = nn.Sequential(\n",
        "            nn.Linear(obs_space_dims, hidden_space1),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_space1, hidden_space2),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "        # Policy Mean specific Linear Layer\n",
        "        self.policy_mean_net = nn.Sequential(\n",
        "            nn.Linear(hidden_space2, action_space_dims)\n",
        "        )\n",
        "\n",
        "        # Policy Std Dev specific Linear Layer\n",
        "        self.policy_stddev_net = nn.Sequential(\n",
        "            nn.Linear(hidden_space2, action_space_dims)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Conditioned on the observation, returns the mean and standard deviation\n",
        "         of a normal distribution from which an action is sampled from.\n",
        "\n",
        "        Args:\n",
        "            x: Observation from the environment\n",
        "\n",
        "        Returns:\n",
        "            action_means: predicted mean of the normal distribution\n",
        "            action_stddevs: predicted standard deviation of the normal distribution\n",
        "        \"\"\"\n",
        "        shared_features = self.shared_net(x.float())\n",
        "\n",
        "        action_means = self.policy_mean_net(shared_features)\n",
        "        action_stddevs = torch.log(\n",
        "            1 + torch.exp(self.policy_stddev_net(shared_features))\n",
        "        )\n",
        "\n",
        "        return action_means, action_stddevs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building an agent\n",
        "\n",
        "<img src=\"file://_static/img/tutorials/reinforce_invpend_gym_v26_fig3.jpeg\">\n",
        "\n",
        "Now that we are done building the policy, let us develop **REINFORCE** which gives life to the policy network.\n",
        "The algorithm of REINFORCE could be found above. As mentioned before, REINFORCE aims to maximize the Monte-Carlo returns.\n",
        "\n",
        "Fun Fact: REINFROCE is an acronym for \" 'RE'ward 'I'ncrement 'N'on-negative 'F'actor times 'O'ffset 'R'einforcement times 'C'haracteristic 'E'ligibility\n",
        "\n",
        "Note: The choice of hyperparameters is to train a decently performing agent. No extensive hyperparameter\n",
        "tuning was done.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class REINFORCE:\n",
        "    \"\"\"REINFORCE algorithm.\"\"\"\n",
        "\n",
        "    def __init__(self, obs_space_dims: int, action_space_dims: int):\n",
        "        \"\"\"Initializes an agent that learns a policy via REINFORCE algorithm [1]\n",
        "        to solve the task at hand (Inverted Pendulum v4).\n",
        "\n",
        "        Args:\n",
        "            obs_space_dims: Dimension of the observation space\n",
        "            action_space_dims: Dimension of the action space\n",
        "        \"\"\"\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.learning_rate = 1e-4  # Learning rate for policy optimization\n",
        "        self.gamma = 0.99  # Discount factor\n",
        "        self.eps = 1e-6  # small number for mathematical stability\n",
        "\n",
        "        self.probs = []  # Stores probability values of the sampled action\n",
        "        self.rewards = []  # Stores the corresponding rewards\n",
        "\n",
        "        self.net = Policy_Network(obs_space_dims, action_space_dims)\n",
        "        self.optimizer = torch.optim.AdamW(self.net.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    def sample_action(self, state: np.ndarray) -> float:\n",
        "        \"\"\"Returns an action, conditioned on the policy and observation.\n",
        "\n",
        "        Args:\n",
        "            state: Observation from the environment\n",
        "\n",
        "        Returns:\n",
        "            action: Action to be performed\n",
        "        \"\"\"\n",
        "        state = torch.tensor(np.array([state]))\n",
        "        action_means, action_stddevs = self.net(state)\n",
        "\n",
        "        # create a normal distribution from the predicted\n",
        "        #   mean and standard deviation and sample an action\n",
        "        distrib = Normal(action_means[0] + self.eps, action_stddevs[0] + self.eps)\n",
        "        action = distrib.sample()\n",
        "        prob = distrib.log_prob(action)\n",
        "\n",
        "        action = action.numpy()\n",
        "\n",
        "        self.probs.append(prob)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"Updates the policy network's weights.\"\"\"\n",
        "        running_g = 0\n",
        "        gs = []\n",
        "\n",
        "        # Discounted return (backwards) - [::-1] will return an array in reverse\n",
        "        for R in self.rewards[::-1]:\n",
        "            running_g = R + self.gamma * running_g\n",
        "            gs.insert(0, running_g)\n",
        "\n",
        "        deltas = torch.tensor(gs)\n",
        "\n",
        "        loss = 0\n",
        "        # minimize -1 * prob * reward obtained\n",
        "        for log_prob, delta in zip(self.probs, deltas):\n",
        "            exit(log_prob)\n",
        "            loss += log_prob.mean() * delta * (-1)\n",
        "\n",
        "        # Update the policy network\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Empty / zero out all episode-centric/related variables\n",
        "        self.probs = []\n",
        "        self.rewards = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets train the policy using REINFORCE to master the task of Inverted Pendulum.\n",
        "\n",
        "Following is the overview of the training procedure\n",
        "\n",
        "   for seed in random seeds\n",
        "       reinitialize agent\n",
        "\n",
        "       for episode in range of max number of episodes\n",
        "           until episode is done\n",
        "               sample action based on current observation\n",
        "\n",
        "               take action and receive reward and next observation\n",
        "\n",
        "               store action take, its probability, and the observed reward\n",
        "           update the policy\n",
        "\n",
        "Note: Deep RL is fairly brittle concerning random seed in a lot of common use cases (https://spinningup.openai.com/en/latest/spinningup/spinningup.html).\n",
        "Hence it is important to test out various seeds, which we will be doing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running build_ext\n",
            "building 'mujoco_py.cymj' extension\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/home/flavio/.local/lib/python3.10/site-packages/mujoco_py -I/home/flavio/.mujoco/mujoco210/include -I/home/flavio/.local/lib/python3.10/site-packages/numpy/core/include -I/usr/include/python3.10 -c /home/flavio/.local/lib/python3.10/site-packages/mujoco_py/cymj.c -o /home/flavio/.local/lib/python3.10/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_310_linuxcpuextensionbuilder/temp.linux-x86_64-3.10/home/flavio/.local/lib/python3.10/site-packages/mujoco_py/cymj.o -fopenmp -w\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/home/flavio/.local/lib/python3.10/site-packages/mujoco_py -I/home/flavio/.mujoco/mujoco210/include -I/home/flavio/.local/lib/python3.10/site-packages/numpy/core/include -I/usr/include/python3.10 -c /home/flavio/.local/lib/python3.10/site-packages/mujoco_py/gl/osmesashim.c -o /home/flavio/.local/lib/python3.10/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_310_linuxcpuextensionbuilder/temp.linux-x86_64-3.10/home/flavio/.local/lib/python3.10/site-packages/mujoco_py/gl/osmesashim.o -fopenmp -w\n",
            "creating /home/flavio/.local/lib/python3.10/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_310_linuxcpuextensionbuilder/lib.linux-x86_64-3.10\n",
            "creating /home/flavio/.local/lib/python3.10/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_310_linuxcpuextensionbuilder/lib.linux-x86_64-3.10/mujoco_py\n",
            "x86_64-linux-gnu-gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 /home/flavio/.local/lib/python3.10/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_310_linuxcpuextensionbuilder/temp.linux-x86_64-3.10/home/flavio/.local/lib/python3.10/site-packages/mujoco_py/cymj.o /home/flavio/.local/lib/python3.10/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_310_linuxcpuextensionbuilder/temp.linux-x86_64-3.10/home/flavio/.local/lib/python3.10/site-packages/mujoco_py/gl/osmesashim.o -L/home/flavio/.mujoco/mujoco210/bin -Wl,--enable-new-dtags,-R/home/flavio/.mujoco/mujoco210/bin -lmujoco210 -lglewosmesa -lOSMesa -lGL -o /home/flavio/.local/lib/python3.10/site-packages/mujoco_py/generated/_pyxbld_2.1.2.14_310_linuxcpuextensionbuilder/lib.linux-x86_64-3.10/mujoco_py/cymj.cpython-310-x86_64-linux-gnu.so -fopenmp\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'patchelf'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/flavio/Code/uni/rl/resources/gym/reinforce_invpend_gym_v26.ipynb Cell 9\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/flavio/Code/uni/rl/resources/gym/reinforce_invpend_gym_v26.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Create and wrap the environment\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/flavio/Code/uni/rl/resources/gym/reinforce_invpend_gym_v26.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(\u001b[39m\"\u001b[39;49m\u001b[39mInvertedPendulum-v4\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/flavio/Code/uni/rl/resources/gym/reinforce_invpend_gym_v26.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m wrapped_env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mwrappers\u001b[39m.\u001b[39mRecordEpisodeStatistics(env, \u001b[39m50\u001b[39m)  \u001b[39m# Records episode-reward\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/flavio/Code/uni/rl/resources/gym/reinforce_invpend_gym_v26.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m total_num_episodes \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m5e3\u001b[39m)  \u001b[39m# Total number of episodes\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:755\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    752\u001b[0m     env_creator \u001b[39m=\u001b[39m env_spec\u001b[39m.\u001b[39mentry_point\n\u001b[1;32m    753\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    754\u001b[0m     \u001b[39m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 755\u001b[0m     env_creator \u001b[39m=\u001b[39m load_env_creator(env_spec\u001b[39m.\u001b[39;49mentry_point)\n\u001b[1;32m    757\u001b[0m \u001b[39m# Determine if to use the rendering\u001b[39;00m\n\u001b[1;32m    758\u001b[0m render_modes: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:553\u001b[0m, in \u001b[0;36mload_env_creator\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Loads an environment with name of style ``\"(import path):(environment name)\"`` and returns the environment creation function, normally the environment class type.\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \n\u001b[1;32m    546\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39m    The environment constructor for the given environment name.\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    552\u001b[0m mod_name, attr_name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 553\u001b[0m mod \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(mod_name)\n\u001b[1;32m    554\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m    555\u001b[0m \u001b[39mreturn\u001b[39;00m fn\n",
            "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/mujoco/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmujoco\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmujoco_env\u001b[39;00m \u001b[39mimport\u001b[39;00m MujocoEnv, MuJocoPyEnv  \u001b[39m# isort:skip\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# ^^^^^ so that user gets the correct error\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# message if mujoco is not installed correctly\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmujoco\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mant\u001b[39;00m \u001b[39mimport\u001b[39;00m AntEnv\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/mujoco/mujoco_env.py:12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspaces\u001b[39;00m \u001b[39mimport\u001b[39;00m Space\n\u001b[1;32m     11\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mmujoco_py\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     14\u001b[0m     MUJOCO_PY_IMPORT_ERROR \u001b[39m=\u001b[39m e\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mujoco_py/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmujoco_py\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbuilder\u001b[39;00m \u001b[39mimport\u001b[39;00m cymj, ignore_mujoco_warnings, functions, MujocoException\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmujoco_py\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgenerated\u001b[39;00m \u001b[39mimport\u001b[39;00m const\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmujoco_py\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmjrenderpool\u001b[39;00m \u001b[39mimport\u001b[39;00m MjRenderPool\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mujoco_py/builder.py:504\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39mlib\u001b[39m.\u001b[39m__fun\n\u001b[1;32m    503\u001b[0m mujoco_path \u001b[39m=\u001b[39m discover_mujoco()\n\u001b[0;32m--> 504\u001b[0m cymj \u001b[39m=\u001b[39m load_cython_ext(mujoco_path)\n\u001b[1;32m    507\u001b[0m \u001b[39m# Trick to expose all mj* functions from mujoco in mujoco_py.*\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mdict2\u001b[39;00m(\u001b[39mobject\u001b[39m):\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mujoco_py/builder.py:110\u001b[0m, in \u001b[0;36mload_cython_ext\u001b[0;34m(mujoco_path)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mImport error. Trying to rebuild mujoco_py.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m mod \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         cext_so_path \u001b[39m=\u001b[39m builder\u001b[39m.\u001b[39;49mbuild()\n\u001b[1;32m    111\u001b[0m         mod \u001b[39m=\u001b[39m load_dynamic_ext(\u001b[39m'\u001b[39m\u001b[39mcymj\u001b[39m\u001b[39m'\u001b[39m, cext_so_path)\n\u001b[1;32m    113\u001b[0m \u001b[39mreturn\u001b[39;00m mod\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mujoco_py/builder.py:226\u001b[0m, in \u001b[0;36mMujocoExtensionBuilder.build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 226\u001b[0m     built_so_file_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_impl()\n\u001b[1;32m    227\u001b[0m     new_so_file_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_so_file_path()\n\u001b[1;32m    228\u001b[0m     move(built_so_file_path, new_so_file_path)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mujoco_py/builder.py:280\u001b[0m, in \u001b[0;36mLinuxCPUExtensionBuilder._build_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m so_file_path \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_build_impl()\n\u001b[1;32m    279\u001b[0m \u001b[39m# Removes absolute paths to libraries. Allows for dynamic loading.\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m fix_shared_library(so_file_path, \u001b[39m'\u001b[39;49m\u001b[39mlibmujoco210.so\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlibmujoco210.so\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    281\u001b[0m fix_shared_library(so_file_path, \u001b[39m'\u001b[39m\u001b[39mlibglewosmesa.so\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlibglewosmesa.so\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    282\u001b[0m \u001b[39mreturn\u001b[39;00m so_file_path\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mujoco_py/builder.py:154\u001b[0m, in \u001b[0;36mfix_shared_library\u001b[0;34m(so_file, name, library_path)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfix_shared_library\u001b[39m(so_file, name, library_path):\n\u001b[1;32m    153\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Used to fixup shared libraries on Linux \"\"\"\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m     subprocess\u001b[39m.\u001b[39;49mcheck_call([\u001b[39m'\u001b[39;49m\u001b[39mpatchelf\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m--remove-rpath\u001b[39;49m\u001b[39m'\u001b[39;49m, so_file])\n\u001b[1;32m    155\u001b[0m     ldd_output \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39mcheck_output([\u001b[39m'\u001b[39m\u001b[39mldd\u001b[39m\u001b[39m'\u001b[39m, so_file])\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m ldd_output:\n",
            "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:364\u001b[0m, in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_call\u001b[39m(\u001b[39m*\u001b[39mpopenargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    355\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run command with arguments.  Wait for command to complete.  If\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39m    the exit code was zero then return, otherwise raise\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[39m    CalledProcessError.  The CalledProcessError object will have the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39m    check_call([\"ls\", \"-l\"])\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     retcode \u001b[39m=\u001b[39m call(\u001b[39m*\u001b[39;49mpopenargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    365\u001b[0m     \u001b[39mif\u001b[39;00m retcode:\n\u001b[1;32m    366\u001b[0m         cmd \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:345\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39m*\u001b[39mpopenargs, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    338\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run command with arguments.  Wait for command to complete or\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[39m    timeout, then return the returncode attribute.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39m    retcode = call([\"ls\", \"-l\"])\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     \u001b[39mwith\u001b[39;00m Popen(\u001b[39m*\u001b[39;49mpopenargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mas\u001b[39;00m p:\n\u001b[1;32m    346\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m             \u001b[39mreturn\u001b[39;00m p\u001b[39m.\u001b[39mwait(timeout\u001b[39m=\u001b[39mtimeout)\n",
            "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[1;32m    968\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[1;32m    969\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m--> 971\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m    972\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m    973\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m    974\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m    975\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m    976\u001b[0m                         errread, errwrite,\n\u001b[1;32m    977\u001b[0m                         restore_signals,\n\u001b[1;32m    978\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m    979\u001b[0m                         start_new_session)\n\u001b[1;32m    980\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    982\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
            "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1863\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     \u001b[39mif\u001b[39;00m errno_num \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1862\u001b[0m         err_msg \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1864\u001b[0m \u001b[39mraise\u001b[39;00m child_exception_type(err_msg)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'patchelf'"
          ]
        }
      ],
      "source": [
        "# Create and wrap the environment\n",
        "env = gym.make(\"InvertedPendulum-v4\")\n",
        "wrapped_env = gym.wrappers.RecordEpisodeStatistics(env, 50)  # Records episode-reward\n",
        "\n",
        "total_num_episodes = int(5e3)  # Total number of episodes\n",
        "# Observation-space of InvertedPendulum-v4 (4)\n",
        "obs_space_dims = env.observation_space.shape[0]\n",
        "# Action-space of InvertedPendulum-v4 (1)\n",
        "action_space_dims = env.action_space.shape[0]\n",
        "rewards_over_seeds = []\n",
        "\n",
        "for seed in [1, 2, 3, 5, 8]:  # Fibonacci seeds\n",
        "    # set seed\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Reinitialize agent every seed\n",
        "    agent = REINFORCE(obs_space_dims, action_space_dims)\n",
        "    reward_over_episodes = []\n",
        "\n",
        "    for episode in range(total_num_episodes):\n",
        "        # gymnasium v26 requires users to set seed while resetting the environment\n",
        "        obs, info = wrapped_env.reset(seed=seed)\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.sample_action(obs)\n",
        "\n",
        "            # Step return type - `tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]`\n",
        "            # These represent the next observation, the reward from the step,\n",
        "            # if the episode is terminated, if the episode is truncated and\n",
        "            # additional info from the step\n",
        "            obs, reward, terminated, truncated, info = wrapped_env.step(action)\n",
        "            agent.rewards.append(reward)\n",
        "\n",
        "            # End the episode when either truncated or terminated is true\n",
        "            #  - truncated: The episode duration reaches max number of timesteps\n",
        "            #  - terminated: Any of the state space values is no longer finite.\n",
        "            done = terminated or truncated\n",
        "\n",
        "        reward_over_episodes.append(wrapped_env.return_queue[-1])\n",
        "        agent.update()\n",
        "\n",
        "        if episode % 1000 == 0:\n",
        "            avg_reward = int(np.mean(wrapped_env.return_queue))\n",
        "            print(\"Episode:\", episode, \"Average Reward:\", avg_reward)\n",
        "\n",
        "    rewards_over_seeds.append(reward_over_episodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot learning curve\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rewards_to_plot = [[reward[0] for reward in rewards] for rewards in rewards_over_seeds]\n",
        "df1 = pd.DataFrame(rewards_to_plot).melt()\n",
        "df1.rename(columns={\"variable\": \"episodes\", \"value\": \"reward\"}, inplace=True)\n",
        "sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\")\n",
        "sns.lineplot(x=\"episodes\", y=\"reward\", data=df1).set(\n",
        "    title=\"REINFORCE for InvertedPendulum-v4\"\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"file://_static/img/tutorials/reinforce_invpend_gym_v26_fig4.png\">\n",
        "\n",
        "Author: Siddarth Chandrasekar\n",
        "\n",
        "License: MIT License\n",
        "\n",
        "## References\n",
        "\n",
        "[1] Williams, Ronald J.. “Simple statistical gradient-following\n",
        "algorithms for connectionist reinforcement learning.” Machine Learning 8\n",
        "(2004): 229-256.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
