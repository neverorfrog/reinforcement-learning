
\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{epstopdf}
\usepackage[table]{xcolor}
\usepackage{matlab}

\sloppy
\epstopdfsetup{outdir=./}
\graphicspath{ {./theory3_images/} }

\begin{document}

\begin{matlabcode}
clear;
w = sym("w",[2,1]);
x = sym("x",[2,1]);

w0 = [0.8;1]; x0 = [1;0]; x1 = [1;0]; x2 = [0;1];
alpha = 0.1; gamma = 0.9;

%probability of executing action 1 in state s
sigma = @(w,x)(1/(1+exp(-dot(w,x)))); 
sigma(w0,x0) %action 1 in state 1
\end{matlabcode}
\begin{matlaboutput}
ans = 0.6900
\end{matlaboutput}


\begin{matlabcode}
policy(2) = sigma(w,x);     %probability of action 1
policy(1) = 1 - policy(2);  %probability of action 0
g1 = gradient(policy(2),w)/policy(2); %gradient of logprob of action 1
g0 = gradient(policy(1),w)/policy(1); %gradient of logprob of action 0
\end{matlabcode}


\begin{par}
\begin{flushleft}
Action selected at first step is 0 with reward 0 (agent goes in exploration) 
\end{flushleft}
\end{par}

\begin{matlabcode}
t = 0; 
G = 0*gamma^0 + 1*gamma^1;
gradient = eval(subs(g0,[w,x],[w0,x0]));
w1 = w0 + alpha*(gamma^t)*G*gradient
\end{matlabcode}
\begin{matlaboutput}
w1 = 2x1    
    0.7379
    1.0000

\end{matlaboutput}
\begin{matlabcode}
sigma(w1,x1)
\end{matlabcode}
\begin{matlaboutput}
ans = 0.6765
\end{matlaboutput}

\begin{par}
\begin{flushleft}
Action selected at second step is 1 with reward 1 (agent selects the most probable action). In fact, since the target value G is higher than in the first iteration
\end{flushleft}
\end{par}

\begin{matlabcode}
t = 1; 
G = 1*gamma^0;
gradient = eval(subs(g1,[w,x],[w1,x1]));
w2 = w1 + alpha*(gamma^t)*G*gradient
\end{matlabcode}
\begin{matlaboutput}
w2 = 2x1    
    0.7670
    1.0000

\end{matlaboutput}
\begin{matlabcode}
sigma(w2,x1)
\end{matlabcode}
\begin{matlaboutput}
ans = 0.6829
\end{matlaboutput}

\end{document}
