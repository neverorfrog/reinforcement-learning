{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b09a0b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "#from .autonotebook import tqdm as notebook_tqdm\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "# from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "# from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d945059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define env\n",
    "env_id = \"CartPole-v1\"\n",
    "#env_id = \"Acrobot-v1\"\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3daca912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/user/1000/app/com.jetbrains.PyCharm-Community/ipykernel_389/3493697535.py:2: DeprecationWarning: The parameter `create_eval_env` is deprecated and will be removed in the future. Please use `EvalCallback` or a custom Callback instead.\n",
      "  ppo_expert = PPO('MlpPolicy', env_id, verbose=1, create_eval_env=True)\n",
      "/run/user/1000/app/com.jetbrains.PyCharm-Community/ipykernel_389/3493697535.py:5: DeprecationWarning: Parameters `eval_env` and `eval_freq` are deprecated and will be removed in the future. Please use `EvalCallback` or a custom Callback instead.\n",
      "  ppo_expert.learn(total_timesteps=3e4, eval_freq=10000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.7     |\n",
      "|    ep_rew_mean     | 20.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 1470     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 24.4       |\n",
      "|    ep_rew_mean          | 24.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 998        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00946904 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.686     |\n",
      "|    explained_variance   | -0.0142    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.12       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.017     |\n",
      "|    value_loss           | 47.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34          |\n",
      "|    ep_rew_mean          | 34          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 897         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011310823 |\n",
      "|    clip_fraction        | 0.0796      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.663      |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 30.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 47.7         |\n",
      "|    ep_rew_mean          | 47.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 850          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090620825 |\n",
      "|    clip_fraction        | 0.0832       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.632       |\n",
      "|    explained_variance   | 0.224        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.3         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0194      |\n",
      "|    value_loss           | 55.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=387.20 +/- 138.17\n",
      "Episode length: 387.20 +/- 138.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 387          |\n",
      "|    mean_reward          | 387          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081633385 |\n",
      "|    clip_fraction        | 0.0567       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.608       |\n",
      "|    explained_variance   | 0.275        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.7         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0144      |\n",
      "|    value_loss           | 63.8         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 64.6     |\n",
      "|    ep_rew_mean     | 64.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 762      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 81.5       |\n",
      "|    ep_rew_mean          | 81.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 760        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00585492 |\n",
      "|    clip_fraction        | 0.0554     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.595     |\n",
      "|    explained_variance   | 0.501      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 15.8       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    value_loss           | 53.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 99          |\n",
      "|    ep_rew_mean          | 99          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 755         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010000195 |\n",
      "|    clip_fraction        | 0.0717      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 0.538       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.2        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00864    |\n",
      "|    value_loss           | 57.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 114          |\n",
      "|    ep_rew_mean          | 114          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 756          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059337458 |\n",
      "|    clip_fraction        | 0.0471       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.562       |\n",
      "|    explained_variance   | 0.539        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.7         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00873     |\n",
      "|    value_loss           | 67.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 131          |\n",
      "|    ep_rew_mean          | 131          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 758          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024717445 |\n",
      "|    clip_fraction        | 0.0084       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.56        |\n",
      "|    explained_variance   | 0.751        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.2         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00251     |\n",
      "|    value_loss           | 41.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074464376 |\n",
      "|    clip_fraction        | 0.0538       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.56        |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.14         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00686     |\n",
      "|    value_loss           | 19.8         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 150      |\n",
      "|    ep_rew_mean     | 150      |\n",
      "| time/              |          |\n",
      "|    fps             | 725      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 168          |\n",
      "|    ep_rew_mean          | 168          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 731          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056254724 |\n",
      "|    clip_fraction        | 0.0453       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.549       |\n",
      "|    explained_variance   | 0.736        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.1         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00571     |\n",
      "|    value_loss           | 44           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 184          |\n",
      "|    ep_rew_mean          | 184          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 737          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047408454 |\n",
      "|    clip_fraction        | 0.0385       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.551       |\n",
      "|    explained_variance   | 0.867        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.86         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.0062      |\n",
      "|    value_loss           | 26.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 205         |\n",
      "|    ep_rew_mean          | 205         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 741         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010846524 |\n",
      "|    clip_fraction        | 0.09        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.554      |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.28        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 19.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 223        |\n",
      "|    ep_rew_mean          | 223        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 746        |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 38         |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00320752 |\n",
      "|    clip_fraction        | 0.0281     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.54      |\n",
      "|    explained_variance   | 0.0587     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.22       |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.00224   |\n",
      "|    value_loss           | 36.6       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013563162 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.536      |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.415       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 3.6         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 241      |\n",
      "|    ep_rew_mean     | 241      |\n",
      "| time/              |          |\n",
      "|    fps             | 729      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elena/PycharmProjects/esercitazioni_RL/venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward expert agent= 500.0 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "#define expert agent\n",
    "ppo_expert = PPO('MlpPolicy', env_id, verbose=1, create_eval_env=True)\n",
    "\n",
    "#train expert\n",
    "ppo_expert.learn(total_timesteps=3e4, eval_freq=10000)\n",
    "\n",
    "#save expert\n",
    "ppo_expert.save(\"ppo_expert\")\n",
    "\n",
    "#evaluate expert\n",
    "mean_reward, std_reward = evaluate_policy(ppo_expert, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward expert agent= {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "480991d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 4)\n",
      "(40000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [00:22<00:00, 1784.40it/s]\n"
     ]
    }
   ],
   "source": [
    "##create expert dataset\n",
    "\n",
    "#empty dataset\n",
    "num_interactions = int(4e4)\n",
    "\n",
    "expert_observations = np.empty((num_interactions,) + env.observation_space.shape)\n",
    "expert_actions = np.empty((num_interactions,) + env.action_space.shape)\n",
    "\n",
    "print(expert_observations.shape)\n",
    "print(expert_actions.shape)\n",
    "\n",
    "#collect experience usign expert policy\n",
    "obs = env.reset()\n",
    "for i in tqdm(range(num_interactions)):\n",
    "    action, _ = ppo_expert.predict(obs, deterministic=True)\n",
    "    expert_observations[i] = obs\n",
    "    expert_actions[i] = action\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a75579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset\n",
    "np.savez_compressed(\n",
    "   \"expert_data\",\n",
    "   expert_actions=expert_actions,\n",
    "   expert_observations=expert_observations,\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce03945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##dataset class\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "\n",
    "class ExpertDataSet(Dataset):\n",
    "\n",
    "    def __init__(self, expert_observations, expert_actions):\n",
    "        self.observations = expert_observations\n",
    "        self.actions = expert_actions\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.observations[index], self.actions[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11af5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_dataset = ExpertDataSet(expert_observations, expert_actions)\n",
    "\n",
    "#split in 80% training and 20%test\n",
    "batch_size = 64\n",
    "train_prop = 0.8\n",
    "train_size = int(train_prop * len(expert_dataset))\n",
    "test_size = len(expert_dataset) - train_size\n",
    "train_expert_dataset, test_expert_dataset = random_split(expert_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = th.utils.data.DataLoader(  dataset=train_expert_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = th.utils.data.DataLoader(  dataset=test_expert_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c7a5b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Define student agent\n",
    "no_cuda = False\n",
    "use_cuda = not no_cuda and th.cuda.is_available()\n",
    "   \n",
    "device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "class StudentAgent:\n",
    "    def __init__(self, env, train_loader, test_loader, learning_rate):\n",
    "        self.env = env\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        \n",
    "        n_inputs = env.observation_space.shape[0]\n",
    "        n_outputs = env.action_space.n\n",
    "        \n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(n_inputs, 16), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(16, n_outputs),\n",
    "            nn.Softmax(dim=-1))\n",
    "        \n",
    "        print(\"policy net: \", self.policy)\n",
    "        \n",
    "        self.loss_criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.optimizer =  optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.num_eval_episodes = 10\n",
    "        \n",
    "    def train(self, num_epochs):\n",
    "        self.policy.train()\n",
    "        self.policy.to(device)\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                obs, expert_action = data.to(device), target.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                obs = obs.float()\n",
    "                student_action = self.policy(obs)\n",
    "                expert_action = expert_action.long()\n",
    "                loss = self.loss_criterion(student_action, expert_action)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            #compute accuracy\n",
    "            train_acc = self.compute_accuracy(self.train_loader)\n",
    "            test_acc = self.compute_accuracy(self.test_loader)\n",
    "            policy_return = self.evaluate_policy(self.num_eval_episodes)\n",
    "            print(\"Epoch {}:\\ttrain accuracy: {}\\ttest accuracy: {}\\tpolicy return:{}\".format(epoch, train_acc, test_acc, policy_return))\n",
    "\n",
    "    def compute_accuracy(self, loader):\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        \n",
    "        self.policy.eval()\n",
    "        test_loss = 0\n",
    "        with th.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                obs, expert_action = data.to(device), target.to(device)\n",
    "                obs = obs.float()\n",
    "            \n",
    "                student_action = self.policy_action(obs)\n",
    "            \n",
    "                total += student_action.size()[0]\n",
    "                correct += sum(student_action==expert_action).item()\n",
    "            \n",
    "        accuracy = 100. * correct/(float)(total)\n",
    "            \n",
    "        return accuracy\n",
    "            \n",
    "        \n",
    "    \n",
    "    def policy_action(self, obs):\n",
    "        policy_act = self.policy(obs)\n",
    "        return th.argmax(policy_act, dim= 1)\n",
    "        \n",
    "    def evaluate_policy(self, num_episodes, render=False):\n",
    "        rewards = []\n",
    "        for ep in range(num_episodes):\n",
    "            done = False\n",
    "            tot_rew = 0\n",
    "            obs = self.env.reset()\n",
    "\n",
    "            while not done:\n",
    "                obs = th.FloatTensor(obs).unsqueeze(0)\n",
    "                action = self.policy_action(obs)\n",
    "                obs, reward, done, info = env.step(action.item())\n",
    "                if render:\n",
    "                    env.render()\n",
    "                tot_rew += reward\n",
    "            rewards.append(tot_rew)\n",
    "        return mean(rewards)\n",
    "    \n",
    "\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "614e3ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy net:  Sequential(\n",
      "  (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (3): Softmax(dim=-1)\n",
      ")\n",
      "Epoch 0:\ttrain accuracy: 92.675\ttest accuracy: 92.675\tpolicy return:500.0\n",
      "Epoch 1:\ttrain accuracy: 94.975\ttest accuracy: 94.975\tpolicy return:500.0\n",
      "Epoch 2:\ttrain accuracy: 95.55\ttest accuracy: 95.55\tpolicy return:500.0\n",
      "Epoch 3:\ttrain accuracy: 96.3875\ttest accuracy: 96.3875\tpolicy return:500.0\n",
      "Epoch 4:\ttrain accuracy: 96.5875\ttest accuracy: 96.5875\tpolicy return:500.0\n",
      "Epoch 5:\ttrain accuracy: 96.7125\ttest accuracy: 96.7125\tpolicy return:500.0\n",
      "Epoch 6:\ttrain accuracy: 96.7375\ttest accuracy: 96.7375\tpolicy return:500.0\n",
      "Epoch 7:\ttrain accuracy: 97.7375\ttest accuracy: 97.7375\tpolicy return:500.0\n",
      "Epoch 8:\ttrain accuracy: 97.4375\ttest accuracy: 97.4375\tpolicy return:500.0\n",
      "Epoch 9:\ttrain accuracy: 97.825\ttest accuracy: 97.825\tpolicy return:500.0\n",
      "Epoch 10:\ttrain accuracy: 97.725\ttest accuracy: 97.725\tpolicy return:500.0\n",
      "Epoch 11:\ttrain accuracy: 95.7\ttest accuracy: 95.7\tpolicy return:500.0\n",
      "Epoch 12:\ttrain accuracy: 98.2\ttest accuracy: 98.2\tpolicy return:500.0\n",
      "Epoch 13:\ttrain accuracy: 98.3\ttest accuracy: 98.3\tpolicy return:500.0\n",
      "Epoch 14:\ttrain accuracy: 96.9375\ttest accuracy: 96.9375\tpolicy return:500.0\n",
      "Epoch 15:\ttrain accuracy: 98.5\ttest accuracy: 98.5\tpolicy return:500.0\n",
      "Epoch 16:\ttrain accuracy: 99.0375\ttest accuracy: 99.0375\tpolicy return:500.0\n",
      "Epoch 17:\ttrain accuracy: 98.4125\ttest accuracy: 98.4125\tpolicy return:500.0\n",
      "Epoch 18:\ttrain accuracy: 98.925\ttest accuracy: 98.925\tpolicy return:500.0\n",
      "Epoch 19:\ttrain accuracy: 99.25\ttest accuracy: 99.25\tpolicy return:500.0\n",
      "Epoch 20:\ttrain accuracy: 97.775\ttest accuracy: 97.775\tpolicy return:500.0\n",
      "Epoch 21:\ttrain accuracy: 99.2125\ttest accuracy: 99.2125\tpolicy return:500.0\n",
      "Epoch 22:\ttrain accuracy: 99.35\ttest accuracy: 99.35\tpolicy return:500.0\n",
      "Epoch 23:\ttrain accuracy: 99.15\ttest accuracy: 99.15\tpolicy return:500.0\n",
      "Epoch 24:\ttrain accuracy: 98.9\ttest accuracy: 98.9\tpolicy return:500.0\n",
      "Epoch 25:\ttrain accuracy: 99.1875\ttest accuracy: 99.1875\tpolicy return:500.0\n",
      "Epoch 26:\ttrain accuracy: 98.525\ttest accuracy: 98.525\tpolicy return:500.0\n",
      "Epoch 27:\ttrain accuracy: 98.3125\ttest accuracy: 98.3125\tpolicy return:500.0\n",
      "Epoch 28:\ttrain accuracy: 98.85\ttest accuracy: 98.85\tpolicy return:500.0\n",
      "Epoch 29:\ttrain accuracy: 98.3625\ttest accuracy: 98.3625\tpolicy return:500.0\n",
      "Epoch 30:\ttrain accuracy: 99.1375\ttest accuracy: 99.1375\tpolicy return:500.0\n",
      "Epoch 31:\ttrain accuracy: 98.6125\ttest accuracy: 98.6125\tpolicy return:500.0\n",
      "Epoch 32:\ttrain accuracy: 99.1625\ttest accuracy: 99.1625\tpolicy return:500.0\n",
      "Epoch 33:\ttrain accuracy: 98.875\ttest accuracy: 98.875\tpolicy return:500.0\n",
      "Epoch 34:\ttrain accuracy: 98.5375\ttest accuracy: 98.5375\tpolicy return:500.0\n",
      "Epoch 35:\ttrain accuracy: 99.1875\ttest accuracy: 99.1875\tpolicy return:500.0\n",
      "Epoch 36:\ttrain accuracy: 98.45\ttest accuracy: 98.45\tpolicy return:500.0\n",
      "Epoch 37:\ttrain accuracy: 99.3\ttest accuracy: 99.3\tpolicy return:500.0\n",
      "Epoch 38:\ttrain accuracy: 99.4\ttest accuracy: 99.4\tpolicy return:500.0\n",
      "Epoch 39:\ttrain accuracy: 98.5375\ttest accuracy: 98.5375\tpolicy return:500.0\n",
      "Epoch 40:\ttrain accuracy: 97.95\ttest accuracy: 97.95\tpolicy return:500.0\n",
      "Epoch 41:\ttrain accuracy: 98.5625\ttest accuracy: 98.5625\tpolicy return:500.0\n",
      "Epoch 42:\ttrain accuracy: 98.9375\ttest accuracy: 98.9375\tpolicy return:500.0\n",
      "Epoch 43:\ttrain accuracy: 99.3625\ttest accuracy: 99.3625\tpolicy return:500.0\n",
      "Epoch 44:\ttrain accuracy: 98.6875\ttest accuracy: 98.6875\tpolicy return:500.0\n",
      "Epoch 45:\ttrain accuracy: 98.7375\ttest accuracy: 98.7375\tpolicy return:500.0\n",
      "Epoch 46:\ttrain accuracy: 99.375\ttest accuracy: 99.375\tpolicy return:500.0\n",
      "Epoch 47:\ttrain accuracy: 97.3625\ttest accuracy: 97.3625\tpolicy return:500.0\n",
      "Epoch 48:\ttrain accuracy: 98.85\ttest accuracy: 98.85\tpolicy return:500.0\n",
      "Epoch 49:\ttrain accuracy: 98.1\ttest accuracy: 98.1\tpolicy return:500.0\n"
     ]
    }
   ],
   "source": [
    "student = StudentAgent(env, train_loader, test_loader, 0.01)\n",
    "student.train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462442b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72e7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
